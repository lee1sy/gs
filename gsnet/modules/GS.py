import torch
import torch.nn as nn
import torch.nn.functional as F
import timm
class UncertaintyNet(nn.Module):
    def __init__(self, input_dim=14, hidden_dim=64, reliability_bias=-2.0, min_reliability=1e-3):
        """
        Args:
            input_dim: è¾“å…¥ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            reliability_bias: åˆå§‹åç½®ï¼Œ-2.0 å¯¹åº”åˆå§‹è¾“å‡ºçº¦ 0.12 (Expert Initialization)
            min_reliability: æœ€å°å¯é æ€§ï¼Œé˜²æ­¢æ­»æ¢¯åº¦
        """
        super(UncertaintyNet, self).__init__()
        
        self.min_reliability = min_reliability

        # 1. ç‰¹å¾æå– MLP
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(), # ä½¿ç”¨ GELU
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU()
        )
        
        # 2. Reliability Head (ç‹¬ç«‹ Linear + Sigmoidï¼ŒExpert Initialization)
        self.reliability_linear = nn.Linear(hidden_dim // 2, 1)
        
        # 3. Offset Head
        self.offset_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, 2),
            nn.Tanh()
        )

        # 4. æ‰§è¡Œåˆå§‹åŒ–
        self._init_weights(reliability_bias)

    def _init_weights(self, reliability_bias):
        """
        Expert Initialization Strategy:
        - MLP layers: Kaiming normal initialization
        - Reliability head: Near-zero weights (std=0.001) + bias=reliability_bias
        """
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        
        # Expert Initialization: Near-zero weights for reliability head
        nn.init.normal_(self.reliability_linear.weight, mean=0.0, std=0.001)
        nn.init.constant_(self.reliability_linear.bias, reliability_bias)

    def forward(self, gaussians):
        features = self.mlp(gaussians)
        
        # è®¡ç®— Reliability
        reliability_logits = self.reliability_linear(features)
        reliability = torch.sigmoid(reliability_logits)
        
        # æ•°å€¼ç¨³å®šå¤„ç† (Soft Clamp)
        reliability = reliability * (1.0 - self.min_reliability) + self.min_reliability
        
        offset = self.offset_head(features) * 0.1
        
        return reliability, offset

class DynamicProjectionLayer(nn.Module):
    def __init__(self):
        super(DynamicProjectionLayer, self).__init__()
        
    def forward(self, gaussians_xyz, visual_feats, extrinsics, intrinsics, offsets):
        B, N, _ = gaussians_xyz.shape
        B_V, C, H, W = visual_feats.shape
        V = B_V // B
        
        img_H = H * 14.0
        img_W = W * 14.0
        
        visual_feats = visual_feats.view(B, V, C, H, W)
        
        ones = torch.ones(B, N, 1, device=gaussians_xyz.device)
        points_3d_homo = torch.cat([gaussians_xyz, ones], dim=2)
        
        # Weighted Mean Pooling across views (replaces Max Pooling)
        # Initialize accumulators for sum of features and sum of valid masks
        sum_features = None
        sum_valid_masks = None
        
        for v in range(V):
            T = extrinsics[:, v, :, :]
            K = intrinsics[:, v, :, :]
            
            points_cam = torch.bmm(T, points_3d_homo.transpose(1, 2)).transpose(1, 2)
            points_2d_homo = torch.bmm(K, points_cam[:, :, :3].transpose(1, 2)).transpose(1, 2)
            
            depth = points_2d_homo[:, :, 2:3].clamp(min=1e-6)
            points_2d = points_2d_homo[:, :, :2] / depth
            
            grid_base = torch.zeros(B, N, 2, device=points_2d.device)
            grid_base[:, :, 0] = 2.0 * points_2d[:, :, 0] / img_W - 1.0
            grid_base[:, :, 1] = 2.0 * points_2d[:, :, 1] / img_H - 1.0
            
            grid_final = grid_base + offsets 
            grid_final = grid_final.unsqueeze(2)
            
            feats_v = visual_feats[:, v, :, :, :]
            
            sampled = F.grid_sample(feats_v, grid_final, align_corners=False)
            sampled = sampled.squeeze(-1).transpose(1, 2)  # [B, N, C]
            
            valid_mask = (depth.squeeze(-1) > 0.1) & \
                         (grid_final[:, :, 0, 0] >= -1) & (grid_final[:, :, 0, 0] <= 1) & \
                         (grid_final[:, :, 0, 1] >= -1) & (grid_final[:, :, 0, 1] <= 1)
            
            # Expand valid_mask to match feature dimensions [B, N, 1]
            valid_mask_expanded = valid_mask.unsqueeze(-1).float()  # [B, N, 1]
            
            # Apply mask to features
            masked_features = sampled * valid_mask_expanded
            
            # Accumulate sum of features and sum of masks
            if sum_features is None:
                sum_features = masked_features
                sum_valid_masks = valid_mask_expanded
            else:
                sum_features = sum_features + masked_features
                sum_valid_masks = sum_valid_masks + valid_mask_expanded
        
        # Weighted Mean: sum_features / (sum_valid_masks + epsilon)
        epsilon = 1e-8
        sampled_features = sum_features / (sum_valid_masks + epsilon)
        
        return sampled_features


class DualCrossAttention(nn.Module):
    def __init__(self, dim=128, heads=4):
        super(DualCrossAttention, self).__init__()
        
        self.attn_g2v = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)
        self.attn_v2g = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)
        
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        
        self.gate = nn.Sequential(
            nn.Linear(dim * 2, dim),
            nn.Sigmoid()
        )
        
    def forward(self, g_emb, v_emb):
        feat_g2v, _ = self.attn_g2v(query=g_emb, key=v_emb, value=v_emb)
        feat_g2v = self.norm1(g_emb + feat_g2v)
        
        feat_v2g, _ = self.attn_v2g(query=v_emb, key=g_emb, value=g_emb)
        feat_v2g = self.norm2(v_emb + feat_v2g)
        
        combined = torch.cat([feat_g2v, feat_v2g], dim=-1)
        gate = self.gate(combined)
        
        fused = feat_g2v * gate + feat_v2g * (1 - gate)
        
        return fused


class NetVLAD(nn.Module):
    def __init__(self, num_clusters=64, dim=128, normalize_input=True):
        super(NetVLAD, self).__init__()
        self.num_clusters = num_clusters
        self.dim = dim
        self.normalize_input = normalize_input
        self.centroids = nn.Parameter(torch.rand(num_clusters, dim))
        self.conv = nn.Conv1d(dim, num_clusters, kernel_size=1, bias=True)
        self.centroids.data.normal_(0, 0.01).clamp_(-2, 2)
        
    def forward(self, x):
        B, N, D = x.shape
        if self.normalize_input:
            x = F.normalize(x, p=2, dim=2)
        soft_assign = self.conv(x.transpose(1, 2)) 
        soft_assign = F.softmax(soft_assign, dim=1)
        x_expanded = x.unsqueeze(2) 
        centroids_expanded = self.centroids.unsqueeze(0).unsqueeze(0) 
        residuals = x_expanded - centroids_expanded 
        soft_assign = soft_assign.transpose(1, 2) 
        vlad = torch.sum(soft_assign.unsqueeze(-1) * residuals, dim=1) 
        vlad = F.normalize(vlad, p=2, dim=2)
        vlad = vlad.view(B, -1)
        vlad = F.normalize(vlad, p=2, dim=1)
        return vlad


class VisualEncoder(nn.Module):
    def __init__(self):
        super(VisualEncoder, self).__init__()
        
        self.backbone = timm.create_model(
            'vit_small_patch14_dinov2.lvd142m', 
            pretrained=True, 
            num_classes=0,
            dynamic_img_size=True
        )
        
        # ğŸ”§ FIXED: Unfreeze all backbone layers for geometry-aware VPR task
        # Previously frozen 9/12 layers (75%), which prevented adaptation to 
        # camera-LiDAR alignment patterns. All layers are now trainable.
        # for i, block in enumerate(self.backbone.blocks):
        #     if i < 9: 
        #         for param in block.parameters():
        #             param.requires_grad = False
        
        self.embed_dim = 384 
        
    def forward(self, x):
        B, C, H, W = x.shape
        
        H_pad = (14 - H % 14) % 14
        W_pad = (14 - W % 14) % 14
        if H_pad > 0 or W_pad > 0:
            x = F.pad(x, (0, W_pad, 0, H_pad))
            
        features = self.backbone.forward_features(x)
        
        patch_tokens = features[:, self.backbone.num_prefix_tokens:, :]
        
        H_new = H + H_pad
        W_new = W + W_pad
        h_grid = H_new // 14
        w_grid = W_new // 14
        
        x = patch_tokens.permute(0, 2, 1).reshape(B, self.embed_dim, h_grid, w_grid)
        
        return x


class GaussianEncoder(nn.Module):
    def __init__(self, gaussian_dim=14, visual_dim=64, hidden_dim=128, 
                 netvlad_clusters=64, output_dim=256, heads=4):
        super(GaussianEncoder, self).__init__()
        self.proj_gaussian = nn.Sequential(
            nn.Linear(gaussian_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True)
        )
        self.proj_visual = nn.Sequential(
            nn.Linear(visual_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True)
        )
        self.dual_attn = DualCrossAttention(dim=hidden_dim, heads=heads)
        
        # ğŸ”¥ MoE Router: è¾“å…¥å‡ ä½•+è§†è§‰ç‰¹å¾ï¼Œè¾“å‡º3ä¸ªæƒé‡ (Geo, Vis, Fused)
        router_input_dim = hidden_dim * 2  # æ‹¼æ¥åçš„å‡ ä½•+è§†è§‰ç‰¹å¾
        self.router = nn.Sequential(
            nn.Linear(router_input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 3),  # è¾“å‡º3ä¸ªæƒé‡
            nn.Softmax(dim=-1)
        )
        
        # ğŸ”¥ CRITICAL FIX: åˆå§‹åŒ–Routeråç½®ï¼Œä½¿Fusionä¸“å®¶åˆå§‹æƒé‡â‰ˆ1.0
        # æ•°å­¦åŸç†: Softmax(logits) = Softmax([bias_g, bias_v, bias_f])
        # ç›®æ ‡: [w_g, w_v, w_f] â‰ˆ [0.01, 0.01, 0.98] (Fusionå ä¸»å¯¼ï¼Œæ¢å¤96%åŸºçº¿æ€§èƒ½)
        # æ–¹æ³•: è®¾ç½®bias = log(desired_weight) - log(sum_exp)
        # ä¸ºäº†æ•°å€¼ç¨³å®šï¼Œä½¿ç”¨æ›´å¤§çš„å·®å¼‚: [-3.0, -3.0, 2.0]
        # è¿™æ ·Softmaxå â‰ˆ [0.0067, 0.0067, 0.9866] â‰ˆ [0, 0, 1]
        # é¢„æœŸæ•ˆæœ: Epoch 0å³æ¢å¤96% Recallï¼Œé¿å…"å¹³å‡å¼ºå¼±ä¸“å®¶"å¯¼è‡´çš„10%æ€§èƒ½ä¸‹é™
        self._init_router_bias()
        
        # ä¸‰ä¸ªç‹¬ç«‹çš„ä¸“å®¶ç¼–ç å™¨
        self.netvlad_geo = NetVLAD(num_clusters=netvlad_clusters, dim=hidden_dim)
        self.netvlad_vis = NetVLAD(num_clusters=netvlad_clusters, dim=hidden_dim)
        self.netvlad_fused = NetVLAD(num_clusters=netvlad_clusters, dim=hidden_dim)
        
        netvlad_output_dim = netvlad_clusters * hidden_dim
        self.bottleneck_geo = nn.Sequential(
            nn.Linear(netvlad_output_dim, output_dim),
            nn.LayerNorm(output_dim)
        )
        self.bottleneck_vis = nn.Sequential(
            nn.Linear(netvlad_output_dim, output_dim),
            nn.LayerNorm(output_dim)
        )
        self.bottleneck_fused = nn.Sequential(
            nn.Linear(netvlad_output_dim, output_dim),
            nn.LayerNorm(output_dim)
        )
    
    def _init_router_bias(self):
        """
        åˆå§‹åŒ–Routeråç½®ï¼Œä½¿Fusionä¸“å®¶åˆå§‹æƒé‡â‰ˆ0.99
        
        ä¿®å¤åŸç†:
        - é»˜è®¤åˆå§‹åŒ–å¯¼è‡´Softmax([0,0,0]) = [0.33, 0.33, 0.33] (å‡åŒ€æƒé‡)
        - å‡åŒ€æƒé‡ä¼š"å¹³å‡"å¼ºå¼±ä¸“å®¶ï¼Œå¯¼è‡´æ€§èƒ½ä»96%é™è‡³86%
        - é€šè¿‡è®¾ç½®bias=[-3.0, -3.0, 2.0]ï¼Œä½¿Softmaxè¾“å‡ºâ‰ˆ[0.007, 0.007, 0.986]
        - è¿™æ ·Fusionä¸“å®¶åœ¨Epoch 0å°±å ä¸»å¯¼ï¼Œç«‹å³æ¢å¤96%åŸºçº¿æ€§èƒ½
        
        æ•°å­¦éªŒè¯:
        Softmax([-3.0, -3.0, 2.0]) = [exp(-3.0), exp(-3.0), exp(2.0)] / sum
                                   = [0.0498, 0.0498, 7.389] / 7.4886
                                   â‰ˆ [0.0067, 0.0067, 0.9866]
        """
        with torch.no_grad():
            router_final_layer = self.router[-2]  # æœ€åä¸€å±‚Linear (åœ¨Softmaxä¹‹å‰)
            
            # åˆå§‹åŒ–åç½®: [Geo, Vis, Fused] = [-3.0, -3.0, 2.0]
            # è¿™ä¼šä½¿Softmaxè¾“å‡º â‰ˆ [0.0067, 0.0067, 0.9866]
            # æ³¨æ„: åˆå§‹åŒ–æ—¶å¯èƒ½è¿˜åœ¨CPUï¼Œä½¿ç”¨bias.dataçš„device
            if router_final_layer.bias is not None:
                device = router_final_layer.bias.data.device
                router_final_layer.bias.data = torch.tensor([-3.0, -3.0, 2.0], 
                                                             dtype=torch.float32, 
                                                             device=device)
            else:
                # å¦‚æœbiasä¸ºNoneï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„Parameter
                router_final_layer.bias = nn.Parameter(torch.tensor([-3.0, -3.0, 2.0], dtype=torch.float32))
            
            # å°†æƒé‡åˆå§‹åŒ–ä¸ºæ¥è¿‘0çš„å°å€¼ï¼Œç¡®ä¿åç½®ä¸»å¯¼åˆå§‹è¡Œä¸º
            # è¿™æ ·å³ä½¿è¾“å…¥å˜åŒ–ï¼Œåˆå§‹æƒé‡åˆ†å¸ƒä¹Ÿä¸»è¦ç”±biaså†³å®š
            nn.init.normal_(router_final_layer.weight, mean=0.0, std=0.01)
            
            # éªŒè¯åˆå§‹åŒ–æ•ˆæœï¼ˆç”¨äºè°ƒè¯•å’Œç¡®è®¤ï¼‰
            # åˆ›å»ºä¸€ä¸ªdummyè¾“å…¥æ¥éªŒè¯Softmaxè¾“å‡º
            if router_final_layer.bias is not None:
                device = router_final_layer.bias.data.device
                dummy_logits = torch.tensor([[-3.0, -3.0, 2.0]], device=device)
                dummy_weights = F.softmax(dummy_logits, dim=-1)
                w_g, w_v, w_f = dummy_weights[0, 0].item(), dummy_weights[0, 1].item(), dummy_weights[0, 2].item()
                print(f"[Router Init] Initial weights: w_g={w_g:.4f}, w_v={w_v:.4f}, w_f={w_f:.4f} (Target: ~[0.007, 0.007, 0.987])")

    def forward(self, gaussians, visual_features, reliability=None):
        # è®¡ç®—ä¸‰ä¸ªç‹¬ç«‹çš„ä¸“å®¶åµŒå…¥
        g_emb = self.proj_gaussian(gaussians)  # Geo expert
        v_emb = self.proj_visual(visual_features)  # Vis expert
        fused_features = self.dual_attn(g_emb, v_emb)  # Fused expert
        
        # Router: æ‹¼æ¥å‡ ä½•+è§†è§‰ç‰¹å¾ï¼Œè¾“å‡º3ä¸ªæƒé‡
        router_input = torch.cat([g_emb.mean(dim=1), v_emb.mean(dim=1)], dim=-1)  # [B, hidden_dim*2]
        router_weights = self.router(router_input)  # [B, 3] -> (w_g, w_v, w_f)
        
        # ä¸‰ä¸ªç‹¬ç«‹çš„ä¸“å®¶æè¿°ç¬¦
        geo_desc = self.netvlad_geo(g_emb)
        geo_desc = self.bottleneck_geo(geo_desc)
        geo_desc = F.normalize(geo_desc, p=2, dim=1)
        
        vis_desc = self.netvlad_vis(v_emb)
        vis_desc = self.bottleneck_vis(vis_desc)
        vis_desc = F.normalize(vis_desc, p=2, dim=1)
        
        fused_desc = self.netvlad_fused(fused_features)
        fused_desc = self.bottleneck_fused(fused_desc)
        fused_desc = F.normalize(fused_desc, p=2, dim=1)
        
        # MoEåŠ æƒæ±‚å’Œ
        w_g, w_v, w_f = router_weights[:, 0:1], router_weights[:, 1:2], router_weights[:, 2:3]
        final_embedding = w_g * geo_desc + w_v * vis_desc + w_f * fused_desc
        final_embedding = F.normalize(final_embedding, p=2, dim=1)
        
        return final_embedding, router_weights, (geo_desc, vis_desc, fused_desc)


class GaussianFusionNet(nn.Module):
    def __init__(self, visual_dim=64, gaussian_dim=14, hidden_dim=128,
                 netvlad_clusters=64, netvlad_dim=128, output_dim=256):
        super(GaussianFusionNet, self).__init__()
        
        self.visual_encoder = VisualEncoder()
        self.uncertainty_net = UncertaintyNet(input_dim=gaussian_dim)
        self.projection_layer = DynamicProjectionLayer()
        
        real_visual_dim = 384 
        
        self.gaussian_encoder = GaussianEncoder(
            gaussian_dim=gaussian_dim, 
            visual_dim=real_visual_dim,
            hidden_dim=hidden_dim,
            netvlad_clusters=netvlad_clusters, 
            output_dim=output_dim, 
            heads=4
        )
        self.output_dim = output_dim
        
    def forward(self, batch):
        gaussians = batch['gaussians']
        images = batch['images']
        extrinsics = batch['extrinsics']
        intrinsics = batch['intrinsics']
        
        B, V, C, H, W = images.shape
        images_flat = images.view(B * V, C, H, W)
        
        visual_feats = self.visual_encoder(images_flat)
        
        reliability, offsets = self.uncertainty_net(gaussians)
        gaussians_xyz = gaussians[:, :, :3]
        
        sampled_visual_feats = self.projection_layer(
            gaussians_xyz, visual_feats, extrinsics, intrinsics, offsets
        )
        
        embedding, router_weights, expert_descriptors = self.gaussian_encoder(gaussians, sampled_visual_feats)
        
        return {
            'embedding': embedding,
            'router_weights': router_weights,  # [B, 3] -> (w_g, w_v, w_f)
            'expert_descriptors': expert_descriptors,  # (geo_desc, vis_desc, fused_desc)
            'reliability': reliability,  # ä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼ˆå¦‚æœlosséœ€è¦ï¼‰
            'offsets': offsets,
            'sampled_visual_feats': sampled_visual_feats,
            'fused_feat': expert_descriptors[2]  # ä½¿ç”¨fused_descä½œä¸ºfused_feat
        }